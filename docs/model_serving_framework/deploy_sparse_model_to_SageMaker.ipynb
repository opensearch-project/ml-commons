{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b32dbef8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "/*\n",
    " * Copyright OpenSearch Contributors\n",
    " * SPDX-License-Identifier: Apache-2.0\n",
    " */"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff4e2d",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  **This script is supposed to be executed at SageMaker Notebook!**\n",
    "\n",
    "## prerequesites\n",
    "- We have setup an **SageMaker Notebook**, the **S3 bucket** to store the bindle, and config their permission\n",
    "\n",
    "## Step 1\n",
    "Use git to clone this file to your SageMaker Notebook instance, and open this run.ipynb at your SageMaker Notebook\n",
    "\n",
    "## Step 2\n",
    "Prepare the model file for SageMaker. Run below code blocks in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir handler\n",
    "!mkdir handler/code\n",
    "!mkdir handler/MAR-INF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile handler/code/requirements.txt\n",
    "sentence-transformers==5.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ff5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile handler/MAR-INF/MANIFEST.json\n",
    "{\n",
    "  \"runtime\": \"python\",\n",
    "  \"model\": {\n",
    "    \"modelName\": \"neuralsparse\",\n",
    "    \"handler\": \"neural_sparse_handler.py\",\n",
    "    \"modelVersion\": \"1.0\",\n",
    "    \"configFile\": \"neural_sparse_config.yaml\"\n",
    "  },\n",
    "  \"archiverVersion\": \"0.9.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile handler/neural_sparse_config.yaml\n",
    "## configs about dynamic batch inference\n",
    "batchSize: 16\n",
    "maxBatchDelay: 5\n",
    "responseTimeout: 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile handler/neural_sparse_handler.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "from sentence_transformers.sparse_encoder import SparseEncoder\n",
    "\n",
    "model_id = os.environ.get(\n",
    "    \"MODEL_ID\", \"opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte\"\n",
    ")\n",
    "max_bs = int(os.environ.get(\"MAX_BS\", 32))\n",
    "trust_remote_code = model_id.endswith(\"gte\")\n",
    "\n",
    "class SparseEncodingModelHandler(BaseHandler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, context):\n",
    "        self.manifest = context.manifest\n",
    "        properties = context.system_properties\n",
    "\n",
    "        # Print initialization parameters\n",
    "        print(f\"Initializing SparseEncodingModelHandler with model_id: {model_id}\")\n",
    "\n",
    "        # load model and tokenizer\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\"))\n",
    "            if torch.cuda.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.model = SparseEncoder(model_id, device=self.device, trust_remote_code=trust_remote_code)\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, requests):\n",
    "        inputSentence = []\n",
    "        batch_idx = []\n",
    "\n",
    "        for request in requests:\n",
    "            request_body = request.get(\"body\")\n",
    "            if isinstance(request_body, bytearray):\n",
    "                request_body = request_body.decode(\"utf-8\")\n",
    "                request_body = json.loads((request_body))\n",
    "            if isinstance(request_body, list):\n",
    "                inputSentence += request_body\n",
    "                batch_idx.append(len(request_body))\n",
    "            else:\n",
    "                inputSentence.append(request_body)\n",
    "                batch_idx.append(1)\n",
    "\n",
    "        return inputSentence, batch_idx\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        inputSentence, batch_idx = self.preprocess(data)\n",
    "        model_output = self.model.encode_document(inputSentence, batch_size=max_bs)\n",
    "        sparse_embedding = list(map(dict,self.model.decode(model_output)))\n",
    "\n",
    "        outputs = [sparse_embedding[s:e]\n",
    "           for s, e in zip([0]+list(itertools.accumulate(batch_idx))[:-1],\n",
    "                           itertools.accumulate(batch_idx))]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de74dbf",
   "metadata": {},
   "source": [
    "Wrap the handler folder to a tarball. And upload it to your S3 bucket.\n",
    "\n",
    "In handler/neural_sparse_handler.py, we define the model loading, pre-process, inference and post-process. We use mixed-precision to accelerate the inference.\n",
    "\n",
    "In handler/neural_sparse_config.yaml, we define some configs for the torch serve (include dynamic micro-batching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf38b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bucket_name = \"your_bucket_name\"\n",
    "os.system(\"tar -czvf neural-sparse-handler.tar.gz -C handler/ .\")\n",
    "os.system(\n",
    "    f\"aws s3 cp neural-sparse-handler.tar.gz s3://{bucket_name}/neural-sparse-handler.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161796c1",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Use SageMaker python SDK to deploy the tarball on a real-time inference endpoint\n",
    "\n",
    "Here we use ml.g5.xlarge. It's a GPU instance with good price-performance.\n",
    "\n",
    "Please modify the region base according to your settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants that can be customized for models\n",
    "model_id = \"opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte\"\n",
    "max_batch_size = \"32\"\n",
    "\n",
    "# constants related to deployment\n",
    "model_name = \"ns-handler\"\n",
    "endpoint_name = \"ns-handler\"\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "initial_instance_count = 1\n",
    "\n",
    "# run this cell\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = boto3.Session()\n",
    "region = sess.region_name\n",
    "smsess = sagemaker.Session(boto_session=sess)\n",
    "\n",
    "envs = {\n",
    "    \"TS_ASYNC_LOGGING\": \"true\",\n",
    "    \"MODEL_ID\": model_id,\n",
    "    \"MAX_BS\": max_batch_size,\n",
    "    \"PRUNE_RATIO\": prune_ratio,\n",
    "}\n",
    "\n",
    "baseimage = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    py_version=\"py312\",\n",
    "    image_scope=\"inference\",\n",
    "    version=\"2.6\",\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    model_data=f\"s3://{bucket_name}/neural-sparse-handler.tar.gz\",\n",
    "    image_uri=baseimage,\n",
    "    role=role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=model_name,\n",
    "    sagemaker_session=smsess,\n",
    "    env=envs,\n",
    ")\n",
    "\n",
    "endpoint_name = endpoint_name\n",
    "predictor = model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    ModelDataDownloadTimeoutInSeconds=3600,\n",
    "    ContainerStartupHealthCheckTimeoutInSeconds=3600,\n",
    "    VolumeSizeInGB=16,\n",
    ")\n",
    "\n",
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0863ed26",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "After we create the endpoint, use some sample request to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "import json\n",
    "\n",
    "body = [\"Currently New York is rainy.\"]\n",
    "amz = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = amz.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "res = response[\"Body\"].read()\n",
    "results = json.loads(res.decode(\"utf8\"))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30128b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "response:\n",
    "```json\n",
    "{'response': [{'has': 0.19832642376422882,\n",
    "   'new': 0.9849710464477539,\n",
    "   'like': 0.20112557709217072,\n",
    "   'now': 0.7473171949386597,\n",
    "   'state': 0.20818853378295898,\n",
    "   'still': 0.26296505331993103,\n",
    "   'going': 0.17759032547473907,\n",
    "   'york': 1.5465646982192993,\n",
    "   'water': 0.5180262327194214,\n",
    "   'present': 0.24726435542106628,\n",
    "   'today': 0.5316043496131897,\n",
    "   'currently': 0.6706798672676086,\n",
    "   'current': 0.9104140996932983,\n",
    "   'dry': 0.2999960780143738,\n",
    "   'rain': 1.3858059644699097,\n",
    "   'weather': 1.4669378995895386,\n",
    "   'climate': 0.392688512802124,\n",
    "   'wet': 1.070887804031372,\n",
    "   'happening': 0.3875649571418762,\n",
    "   'ny': 1.4108916521072388,\n",
    "   'brooklyn': 0.2983669638633728,\n",
    "   'yorkshire': 0.15651951730251312,\n",
    "   'manhattan': 0.969535231590271,\n",
    "   'flood': 0.2403770089149475,\n",
    "   'flooding': 0.4161500036716461,\n",
    "   'rainfall': 0.9889746904373169,\n",
    "   'damp': 0.38938602805137634,\n",
    "   'moist': 0.32199856638908386,\n",
    "   'mist': 0.2026219218969345,\n",
    "   'precipitation': 0.5729197263717651,\n",
    "   'drought': 0.41227778792381287,\n",
    "   'rains': 0.8187123537063599,\n",
    "   'rainy': 1.4709837436676025,\n",
    "   'nyc': 1.308121681213379,\n",
    "   'yorker': 0.6350979804992676,\n",
    "   'monsoon': 0.6218147873878479,\n",
    "   'raining': 0.9827804565429688,\n",
    "   'cloudy': 0.6314691305160522,\n",
    "   'nyu': 0.7196483612060547}],\n",
    " 'tokens': [{'inputTokens': 8, 'outputTokens': 0}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33035889",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "> **_NOTE:_**  **This step is supposed to be executed at an instance have access to OpenSearch cluster!**\n",
    "\n",
    "Register this SageMaker endpoint at your OpenSearch cluster\n",
    "\n",
    "Please check the OpenSearch doc for more information. Here we provide one demo request body using access_key and secret_key. Please choose the authentication according to your use case.\n",
    "\n",
    "### create connector\n",
    "\n",
    "(Fill the region and predictor.endpoint_name in request body)\n",
    "```json\n",
    "POST /_plugins/_ml/connectors/_create\n",
    "{\n",
    "  \"name\": \"test\",\n",
    "  \"description\": \"Test connector for Sagemaker model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"access_key\": \"your access key\",\n",
    "    \"secret_key\": \"your secret key\"\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"{region}\",\n",
    "    \"service_name\": \"sagemaker\",\n",
    "    \"input_docs_processed_step_size\": 2,\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"url\": \"https://runtime.sagemaker.{region}.amazonaws.com/endpoints/{predictor.endpoint_name}/invocations\",\n",
    "      \"request_body\": \"${parameters.input}\"\n",
    "    }\n",
    "  ],\n",
    "  \"client_config\":{\n",
    "      \"max_retry_times\": -1,\n",
    "      \"max_connection\": 60,\n",
    "      \"retry_backoff_millis\": 10\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### register model\n",
    "```json\n",
    "POST /_plugins/_ml/models/_register?deploy=true\n",
    "{\n",
    "  \"name\": \"test\",\n",
    "  \"function_name\": \"remote\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"connector_id\": \"{connector id}\",\n",
    "  \"description\": \"Test connector for Sagemaker model\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fda443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
